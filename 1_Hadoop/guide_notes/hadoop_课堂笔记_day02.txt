Hadoop的组成:
   hadoop 1.x :  HDFS(存)  + MapReduce(算+资源(内存、CPU、磁盘、网络...)调度) 
   hadoop 2.x/3.x :   HDFS(存) + MapReduce(算) + Yarn(资源调度)、

HDFS的架构:
    HDFS: Hadoop分布式文件系统， 文件系统是用于对文件进行存储和管理。
          分布式可以理解为由多台机器共同构成一个完整的文件系统。
    NameNode(nn): 负责管理HDFS中所有文件的元数据信息.  
                  元数据: 用于描述真实数据的数据就是所谓的元数据.
		          例如: 一个真实的数据文件  a.txt
			        它的元数据为: 文件名 文件大小 文件权限  文件的目录结构  文件对应的块  在哪个dn存 
                 
		  注意:  要想找到HDFS的真实数据必须通过NameNode所维护的元数据才能定位到DataNode中存储的真实数据

    DataNode(dn): 负责管理HDFS的所有的真实文件数据

    SecondaryNameNode(2nn): 辅助NameNode工作. 分担NameNode一些工作，减轻NameNode的压力.
                            注意: 2nn 不是 nn的热备.顶多算nn的秘书.
    
    在一个集群中(非高可用集群): NN(1个)   DN(多个)   2NN(1个)
         

Yarn的架构:
     Yarn: 资源调度和管理的框架。 管理和调度的资源就是整个Hadoop集群的资源
     ResourceManager(RM):  是Yarn的大哥. 负责管理和调度整个集群的资源. 负责处理客户端的请求.
                           负责为Job启动ApplicationMaster。

     NodeManager(NM):      是每台机器资源的管理者。实际上只是将本机器的资源对ResourceManager做一个汇报.
                           对于资源的分配必须听从ResourceManager的指令.

     ApplicationMaster(AM):对应每一个Job(MapReduce程序),负责为Job向ResourceMananger去申请资源，
                           申请到资源以后，负责告诉NodeManager去运行对应的任务. 并且负责监控任务的
			   运行状态和任务的容错.
     Container: 对多维度资源的封装. 方便管理资源及防止资源被侵占.

     简单模拟一个任务的提交过程和资源调度过程：

     首先有一个Job(MapReduce程序): 包含2个MapTask  和 1 个ReduceTask

     1. 客户端提交Job到ResourceManager
     2. ResourceManager 为 Job 启动 ApplicationMaster , ApplicationMaster的运行也需要资源，因此ApplicationMaster
        启动起来以后，就会有一个Container封装ApplicationMaster运行所用的资源. 因为资源都是在NodeManager上，
	所以ApplicationMaster是运行在某一个NodeManager上.
     3. ApplicationMaster 会根据Job的情况向 ResourceManager申请资源来运行每个Task，当前Job总共有3个Task，
        每个Task都是单独运行，因此需要申请3份资源, 也就意味着又有3个Container运行.
     4. 所有的资源的分配都是ResourceManager下达指令给NodeManager进行分配的.
     5. ApplicationMaster为Job成功申请到资源以后，会告诉NodeManager去运行对应的Task 
        每个Task可能运行到不同的机器，也有可能多个Task运行到同一个机器. 要看当时集群的资源情况.
     6. 当Job的每个Task都开始运行，ApplicationMaster负责监控整个Job的状态. 要负责容错相关的事情.
     7. 当Job的每个Task都执行成功后，意味着Job运行完成，此时ApplicationMaster会找ResourceMananger
        申请注销自己，所有为当前Job申请的资源得到释放. 

                                                              申请程序员(申请资源)
                                                             <------                            NodeManager (运行不同的Task)
  客户端(甲方)---> 找你们做项目(提交Job)---> 项目总监(RM) ------->  交给项目经理(AM)  -----> 做项目(程序员负责不同的模块) ---->项目完成(项目经理找项目总监归还程序员)


MapReduce的架构:
   Map阶段(MapTask):  负责将数据分到多台机器中，进行并行计算
   Reduce阶段(ReduceTask): 负责将多台机器在map阶段中计算出来的数据，进行整体的汇总.




Hadoop环境的准备:

1. 准备模板机(最小化安装)
   1.1  安装必要的组件  
        epel-release   psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop
   1.2  修改/etc/hosts文件
        192.168.202.101 hadoop101
	......
   1.3  设置防火墙不自启
        systemctl stop firewalld
	systemctl disable firewalld
   1.4  创建atguigu用户
   1.5  配置atguigu用户具有root权限
   1.6  在/opt目录下创建  software  和  module 两个目录.
        并且将 software和 module两个目录的 所属主 和 所属组 都改为atguigu.



2. 准备具体使用的机器: 
  
   2.1 基于模板机克隆虚拟机
   2.2 修改主机名
   2.3 修改ip地址


3. 安装JDK 和 配置环境变量
  3.1 通过xftp将jdk的安装包上传到Linux中的/opt/software目录下
  3.2 解压到/opt/module目录下
  3.3 配置环境变量
 
      1）. 阅读 /etc/profile文件中的注释
           # It's NOT a good idea to change this file unless you know what you
	   # are doing. It's much better to create a custom.sh shell script in
           # /etc/profile.d/ to make custom changes to your environment, as this
           # will prevent the need for merging in future updates.

      2）. 在 /etc/profile.d 目录下创建 my_env.sh
           sudo  touch  /etc/profile.d/my_env.sh

      3）. 编辑my_env.sh 配置jdk的环境变量
           sudo vim /etc/profile.d/my_env.sh
	   
	   加入如下内容: 	
	   #JAVA_HOME
	   JAVA_HOME=/opt/module/jdk1.8.0_212
           PATH=$PATH:$JAVA_HOME/bin

           # export  all system  variable  and user variable to global
           export PATH JAVA_HOME

      4）. 配置完后， 需要通过 source  /etc/profile 文件让环境变量生效.
           如果不生效，建议仔细查看是否配置正确。
	   如果正确，可尝试关闭xshell窗口，重新打开窗口 或者直接重启linux

	   测试是否生效:
	   echo $JAVA_HOME
	   echo $PATH

4. 安装Hadoop及配置环境变量
  4.1 通过xftp将hadoop的安装包上传到Linux中的/opt/software目录下
  4.2 解压到/opt/module目录下
  4.3 配置环境变量

      1）. 编辑my_env.sh 配置hadoop的环境变量
           sudo vim /etc/profile.d/my_env.sh
	   
	   加入如下内容: 	
	   #JAVA_HOME
	   JAVA_HOME=/opt/module/jdk1.8.0_212
	   #HADOOP_HOME
	   HADOOP_HOME=/opt/module/hadoop-3.1.3

	   PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

           # export  all system  variable  and user variable to global
           export PATH JAVA_HOME HADOOP_HOME


      2）. 配置完后， 需要通过 source  /etc/profile 文件让环境变量生效.
           如果不生效，建议仔细查看是否配置正确。
	   如果正确，可尝试关闭xshell窗口，重新打开窗口 或者直接重启linux

	   测试是否生效:
	   echo $HADOOP_HOME
	   echo $PATH


Hadoop运行模式

1. 本地模式 
   1.1 不需要进行任何的hadoop相关的配置就能够直接运行，因为Hadoop默认的配置就是本地模式.
   1.2 测试两个官方的案例:
       grep: 
       hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar grep input output 'dfs[a-z]+'

       wordcount:
       hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount wcinput wcoutput
   1.3 需要注意的问题:
       1）.需要注意指定时路径的问题。
       2）.需要注意输出路径存在的问题
           指定的输出路径不能存在，如果存在，则抛出异常。
	   org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/opt/module/hadoop-3.1.3/wcoutput already exists


2. 伪分布式模式

3. 完全分布式模式



作业: 
1. 总结梳理 Hadoop的组成  
	     HDFS的架构
	     Yarn的架构
	     MapReduce的架构
2. Hadoop的环境的搭建
   1. 准备模板机，方便后续使用
   2. 克隆搭建Hadoop环境的机器，修改主机名，ip等
   3. 安装JDK 配置环境变量
   4. 安装Hadoop 配置环境变量
   5. 测试Hadoop运行模式之 本地模式(两个官方案例)

3. 回顾最近讲解的内容

